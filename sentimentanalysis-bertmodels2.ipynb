{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"In this project we'll be performing sentiment analysis on Rotten Tomatoes Dataset whose dataset has been attached in this repo\n\nThe main task corresponds to a multi-class text classification on Movie Reviews Competition and the dataset contains 156,060 from which we have to classify among 5 classes. The sentiment labels are:\n\n0 → Negative      </br>\n1 → Somewhat negative  </br>\n2 → Neutral </br>\n3 → Somewhat positive </br>\n4 → Positive </br>\n\n\n\nWe will be comparing performance of several algorithms and will deduce which works best","metadata":{}},{"cell_type":"markdown","source":"# Steps to be followed\n\n1. Importing necessary libraries\n2. Opening the train and test dataset in the form of pandas dataframe and perform exploratory data analysis on train data\n3. Performing pre-processing \n4. Taking the train data and splitting it into train and val dataset ( test set is already given)\n5. Applying different models </br>\n   a) BERT </br>\n   b) RoBERTa (Robustly Optimized BERT Pre-training Approach) </br>\n5. Comparing performance of different models \n6. Final Analysis","metadata":{}},{"cell_type":"markdown","source":"### Step 1. Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## other libraries will be imported as and when required","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:13.438931Z","iopub.execute_input":"2022-02-02T07:09:13.439191Z","iopub.status.idle":"2022-02-02T07:09:13.444068Z","shell.execute_reply.started":"2022-02-02T07:09:13.439154Z","shell.execute_reply":"2022-02-02T07:09:13.443282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Opening the dataset and performing EDA","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/sentimentdata/train.tsv/train.tsv' , sep='\\t')\ntest_df = pd.read_csv('../input/sentimentdata/test.tsv/test.tsv' , sep = '\\t')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:13.611832Z","iopub.execute_input":"2022-02-02T07:09:13.612081Z","iopub.status.idle":"2022-02-02T07:09:14.210732Z","shell.execute_reply.started":"2022-02-02T07:09:13.612053Z","shell.execute_reply":"2022-02-02T07:09:14.21001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:14.213708Z","iopub.execute_input":"2022-02-02T07:09:14.213918Z","iopub.status.idle":"2022-02-02T07:09:14.230709Z","shell.execute_reply.started":"2022-02-02T07:09:14.213886Z","shell.execute_reply":"2022-02-02T07:09:14.229931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\n#print(train_df.info)\nprint(train_df.columns)\nprint(train_df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:14.238583Z","iopub.execute_input":"2022-02-02T07:09:14.23877Z","iopub.status.idle":"2022-02-02T07:09:14.262079Z","shell.execute_reply.started":"2022-02-02T07:09:14.238748Z","shell.execute_reply":"2022-02-02T07:09:14.261291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Sentiment'].value_counts()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:14.469179Z","iopub.execute_input":"2022-02-02T07:09:14.469379Z","iopub.status.idle":"2022-02-02T07:09:14.48226Z","shell.execute_reply.started":"2022-02-02T07:09:14.469355Z","shell.execute_reply":"2022-02-02T07:09:14.481512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=train_df.copy(deep=True)\npie1=pd.DataFrame(df2['Sentiment'].replace(0,'Negative').replace(1,'Somewhat negative').replace(2,'Neutral').replace(3,'Somewhat positive').replace(4,'Positive').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Sentiment Class',y = 'Sentiment', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:14.662751Z","iopub.execute_input":"2022-02-02T07:09:14.663486Z","iopub.status.idle":"2022-02-02T07:09:14.985617Z","shell.execute_reply.started":"2022-02-02T07:09:14.663434Z","shell.execute_reply":"2022-02-02T07:09:14.984797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Insights*** <br>\nThere is an imbalance . So we cannot do random split, We'll do <tt>**StratifiedSplit()**</tt> to ensure distribution is same in splits","metadata":{}},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5,figsize=(25,8))\nax1.hist(train_df[train_df['Sentiment'] == 0]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='b')\nax1.set_title('Negative Reviews')\n\nax2.hist(train_df[train_df['Sentiment'] == 1]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='r')\nax2.set_title('Somewhat Negative Reviews')\n\nax3.hist(train_df[train_df['Sentiment'] == 2]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='g')\nax3.set_title('Neutral Reviews')\n\nax4.hist(train_df[train_df['Sentiment'] == 3]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='y')\nax4.set_title('Somewhat Positive Reviews')\n\nax5.hist(train_df[train_df['Sentiment'] == 4]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='k')\nax5.set_title('Positive Reviews')\n\nf.suptitle('Histogram number of words in reviews')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:15.323307Z","iopub.execute_input":"2022-02-02T07:09:15.32419Z","iopub.status.idle":"2022-02-02T07:09:16.905931Z","shell.execute_reply.started":"2022-02-02T07:09:15.324144Z","shell.execute_reply":"2022-02-02T07:09:16.905284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Phrase'].str.split().map(lambda x: len(x)).max()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:16.90752Z","iopub.execute_input":"2022-02-02T07:09:16.907969Z","iopub.status.idle":"2022-02-02T07:09:17.610925Z","shell.execute_reply.started":"2022-02-02T07:09:16.907933Z","shell.execute_reply":"2022-02-02T07:09:17.610256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Insights*** <br>\nThrough these graphs we can see that most reviews of any class are of shorter length, around 5-20. But max length is 52 \nEffectively was 52 words, this means if we would Tokenize by word the max_length should be 52, however as transformers consider sub-words tokenization such number could be increased depending on the words being used which can increase such length to 60 or even more, thus we have to take that into account when modeling as it could cause our model to take significatively a long time to train, therefore we have to find a trade-off between training time and performance.","metadata":{}},{"cell_type":"code","source":"\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=20)\nprint('Number of sentences which contain more than 20 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=30)\nprint('Number of sentences which contain more than 30 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=40)\nprint('Number of sentences which contain more than 40 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=50)\nprint('Number of sentences which contain more than 50 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))==52)\nprint('Number of sentences which contain 52 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\n#dfff.loc[dfff['Phrase']==True]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:17.615069Z","iopub.execute_input":"2022-02-02T07:09:17.618294Z","iopub.status.idle":"2022-02-02T07:09:19.888284Z","shell.execute_reply.started":"2022-02-02T07:09:17.618251Z","shell.execute_reply":"2022-02-02T07:09:19.887403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Insights*** <br>\nWe can remove sentences which have length more than 40 words and they won't contribute much but removing them can help to boost computation","metadata":{}},{"cell_type":"code","source":"train_df['len'] = train_df['Phrase'].str.split().map(lambda x: len(x))\nprint(train_df.shape)\n\ntrain_df = train_df[train_df['len'] <40 ]\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:19.890123Z","iopub.execute_input":"2022-02-02T07:09:19.890447Z","iopub.status.idle":"2022-02-02T07:09:20.415924Z","shell.execute_reply.started":"2022-02-02T07:09:19.890407Z","shell.execute_reply":"2022-02-02T07:09:20.415161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"156060 - 155708","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:20.417031Z","iopub.execute_input":"2022-02-02T07:09:20.417267Z","iopub.status.idle":"2022-02-02T07:09:20.425922Z","shell.execute_reply.started":"2022-02-02T07:09:20.417234Z","shell.execute_reply":"2022-02-02T07:09:20.425118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 4. Taking the new data and splitting it into train and test (validation set will be made from train set later)","metadata":{}},{"cell_type":"code","source":"train_df['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:20.428519Z","iopub.execute_input":"2022-02-02T07:09:20.429191Z","iopub.status.idle":"2022-02-02T07:09:20.438173Z","shell.execute_reply.started":"2022-02-02T07:09:20.429157Z","shell.execute_reply":"2022-02-02T07:09:20.437226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is an imbalance . So we cannot do random split, We'll do <tt>**StratifiedSplit()**</tt> to ensure distribution is same in splits","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nX = train_df.drop('Sentiment',axis=1)\ny = train_df['Sentiment']\nsss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0) #test size of 10% \n\nfor train_index, test_index in sss.split(X , y):  \n    X_train = X.iloc[train_index]\n    y_train = y.iloc[train_index]  \n    X_val = X.iloc[test_index]\n    y_val = y.iloc[test_index]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:20.43968Z","iopub.execute_input":"2022-02-02T07:09:20.440008Z","iopub.status.idle":"2022-02-02T07:09:20.714087Z","shell.execute_reply.started":"2022-02-02T07:09:20.439971Z","shell.execute_reply":"2022-02-02T07:09:20.713371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train distribution')\nprint(y_train.value_counts())\nprint(y_train.shape[0])\nprint(\"\\n\")\nprint('Val distribution')\nprint(y_val.value_counts())\nprint(y_val.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:20.715205Z","iopub.execute_input":"2022-02-02T07:09:20.715434Z","iopub.status.idle":"2022-02-02T07:09:20.727968Z","shell.execute_reply.started":"2022-02-02T07:09:20.715402Z","shell.execute_reply":"2022-02-02T07:09:20.727229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:20.728827Z","iopub.execute_input":"2022-02-02T07:09:20.729004Z","iopub.status.idle":"2022-02-02T07:09:25.706292Z","shell.execute_reply.started":"2022-02-02T07:09:20.728976Z","shell.execute_reply":"2022-02-02T07:09:25.705367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 5. Applying Transformer Model\n### a) BERT","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , ModelCheckpoint , EarlyStopping\n\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:25.711027Z","iopub.execute_input":"2022-02-02T07:09:25.711354Z","iopub.status.idle":"2022-02-02T07:09:25.729266Z","shell.execute_reply.started":"2022-02-02T07:09:25.711317Z","shell.execute_reply":"2022-02-02T07:09:25.728499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TFBertModel,  BertConfig, BertTokenizerFast\n\nmodel_name = 'bert-base-uncased'\nmax_length = 45\n\nconfig = BertConfig.from_pretrained(model_name)\nbert_tokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path=model_name , config=config)\ntransformer_bert_model = TFBertModel.from_pretrained(model_name , config = config)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:26.746697Z","iopub.execute_input":"2022-02-02T07:09:26.747248Z","iopub.status.idle":"2022-02-02T07:09:51.929608Z","shell.execute_reply.started":"2022-02-02T07:09:26.747208Z","shell.execute_reply":"2022-02-02T07:09:51.928868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_text = train_df['Phrase'][0]\nprint(sample_text)\nprint(bert_tokenizer(sample_text))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:51.931522Z","iopub.execute_input":"2022-02-02T07:09:51.931983Z","iopub.status.idle":"2022-02-02T07:09:51.953833Z","shell.execute_reply.started":"2022-02-02T07:09:51.931946Z","shell.execute_reply":"2022-02-02T07:09:51.953136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building the model","metadata":{}},{"cell_type":"code","source":"\n\ninput_ids = Input(shape = (max_length,) , name = 'input_ids' , dtype = 'int32')\n\n#transformer_bert_model.trainable = False\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = transformer_bert_model(input_ids)[1]\n\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n\n# Then build your model output\nSentiments = Dense(units=5, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = Sentiments\nbert_model = Model(inputs=input_ids, outputs=outputs, name='Bert-SentimentNetwork')\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:51.954884Z","iopub.execute_input":"2022-02-02T07:09:51.955113Z","iopub.status.idle":"2022-02-02T07:09:58.436822Z","shell.execute_reply.started":"2022-02-02T07:09:51.955079Z","shell.execute_reply":"2022-02-02T07:09:58.436044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:58.441748Z","iopub.execute_input":"2022-02-02T07:09:58.444509Z","iopub.status.idle":"2022-02-02T07:09:58.477544Z","shell.execute_reply.started":"2022-02-02T07:09:58.444452Z","shell.execute_reply":"2022-02-02T07:09:58.47689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\nbert_model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n\n\nx_train = bert_tokenizer(\n          text=X_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n\n\nx_val = bert_tokenizer(\n          text=X_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:09:58.478919Z","iopub.execute_input":"2022-02-02T07:09:58.479152Z","iopub.status.idle":"2022-02-02T07:10:08.315033Z","shell.execute_reply.started":"2022-02-02T07:09:58.479119Z","shell.execute_reply":"2022-02-02T07:10:08.314266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# callbacks = [\n#     EarlyStopping(patience=5),\n#     ReduceLROnPlateau(factor=0.3, patience=3, min_lr=0.00001 ),\n#     ModelCheckpoint('bert_model.h5')\n# ]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:10:08.316505Z","iopub.execute_input":"2022-02-02T07:10:08.316765Z","iopub.status.idle":"2022-02-02T07:10:08.320648Z","shell.execute_reply.started":"2022-02-02T07:10:08.31673Z","shell.execute_reply":"2022-02-02T07:10:08.319958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the model\nhistory = bert_model.fit(\n    x=x_train['input_ids'],\n    y= y_train,\n    validation_data=(x_val['input_ids'], y_val),\n    batch_size=256,\n    epochs=10,\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-02T07:11:12.676598Z","iopub.execute_input":"2022-02-02T07:11:12.676872Z","iopub.status.idle":"2022-02-02T07:11:21.834974Z","shell.execute_reply.started":"2022-02-02T07:11:12.676841Z","shell.execute_reply":"2022-02-02T07:11:21.833811Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred = bert_model.predict(x_val['input_ids'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:12:40.425513Z","iopub.execute_input":"2022-02-01T12:12:40.426298Z","iopub.status.idle":"2022-02-01T12:13:23.883499Z","shell.execute_reply.started":"2022-02-01T12:12:40.426251Z","shell.execute_reply":"2022-02-01T12:13:23.882736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:13:42.122579Z","iopub.execute_input":"2022-02-01T12:13:42.123294Z","iopub.status.idle":"2022-02-01T12:13:42.128745Z","shell.execute_reply.started":"2022-02-01T12:13:42.123253Z","shell.execute_reply":"2022-02-01T12:13:42.127992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred_max = np.argmax(y_val_pred , axis = 1)\ny_val_gt_max = np.argmax(y_val , axis = 1)\n\nprint(y_val_pred_max.shape)\nprint(y_val_gt_max.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:14:04.097817Z","iopub.execute_input":"2022-02-01T12:14:04.098078Z","iopub.status.idle":"2022-02-01T12:14:04.104567Z","shell.execute_reply.started":"2022-02-01T12:14:04.098049Z","shell.execute_reply":"2022-02-01T12:14:04.103881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nreport = classification_report(y_val_pred_max, y_val_gt_max)\n\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:14:39.730467Z","iopub.execute_input":"2022-02-01T12:14:39.730765Z","iopub.status.idle":"2022-02-01T12:14:39.758164Z","shell.execute_reply.started":"2022-02-01T12:14:39.730731Z","shell.execute_reply":"2022-02-01T12:14:39.757506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nprint(sns.heatmap(confusion_matrix(y_val_gt_max , y_val_pred_max) , annot=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:15:09.132903Z","iopub.execute_input":"2022-02-01T12:15:09.133386Z","iopub.status.idle":"2022-02-01T12:15:09.422442Z","shell.execute_reply.started":"2022-02-01T12:15:09.133346Z","shell.execute_reply":"2022-02-01T12:15:09.421078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b) RoBERTa","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig \n\nmodel_name = 'roberta-base'\nmax_length = 40\n\nconfig = RobertaConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\nroberta_tokenizer = RobertaTokenizer.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\ntransformer_roberta_model = TFRobertaModel.from_pretrained(model_name, config = config)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:15:44.627412Z","iopub.execute_input":"2022-02-01T12:15:44.627707Z","iopub.status.idle":"2022-02-01T12:16:05.237264Z","shell.execute_reply.started":"2022-02-01T12:15:44.627663Z","shell.execute_reply":"2022-02-01T12:16:05.236533Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = Input(shape = (max_length,) , name = 'input_ids' , dtype = 'int32')\n\n#transformer_roberta_model.trainable = False\n# Load the Transformers RoBERTa model as a layer in a Keras model\nroberta_model = transformer_roberta_model(input_ids)[1]\n\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(roberta_model, training=False)\n\n\n# Then build your model output\nSentiments = Dense(units=5, kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='Sentiment')(pooled_output)\noutputs = Sentiments\nroberta_model = Model(inputs=input_ids, outputs=outputs, name='RobBERTa_Sentiment')","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:16:07.972129Z","iopub.execute_input":"2022-02-01T12:16:07.97275Z","iopub.status.idle":"2022-02-01T12:17:01.541467Z","shell.execute_reply.started":"2022-02-01T12:16:07.972709Z","shell.execute_reply":"2022-02-01T12:17:01.540521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"roberta_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:17:01.543546Z","iopub.execute_input":"2022-02-01T12:17:01.543849Z","iopub.status.idle":"2022-02-01T12:17:01.565053Z","shell.execute_reply.started":"2022-02-01T12:17:01.543809Z","shell.execute_reply":"2022-02-01T12:17:01.564347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\nloss = {'Sentiment': CategoricalCrossentropy(from_logits = True)}\nroberta_model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\nx_train = roberta_tokenizer(\n          text=X_train['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)\n\n\n\nx_val = roberta_tokenizer(\n          text=X_val['Phrase'].to_list(),\n          add_special_tokens=True,\n          max_length=max_length,\n          truncation=True,\n          padding=True, \n          return_tensors='tf',\n          return_token_type_ids = False,\n          return_attention_mask = True,\n          verbose = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:17:01.566366Z","iopub.execute_input":"2022-02-01T12:17:01.56674Z","iopub.status.idle":"2022-02-01T12:17:27.039638Z","shell.execute_reply.started":"2022-02-01T12:17:01.566704Z","shell.execute_reply":"2022-02-01T12:17:27.038871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [\n    EarlyStopping(patience=5, verbose=1 , monitor = 'accuracy'),\n    ReduceLROnPlateau(factor=0.3, patience=3, min_lr=0.00001),\n    ModelCheckpoint('roberta_model.h5',mode='max', verbose=1,monitor=\"val_accuracy\" ,save_best_only=True, save_weights_only=False)\n]","metadata":{"execution":{"iopub.status.busy":"2022-02-01T13:20:53.414908Z","iopub.execute_input":"2022-02-01T13:20:53.415251Z","iopub.status.idle":"2022-02-01T13:20:53.496909Z","shell.execute_reply.started":"2022-02-01T13:20:53.415165Z","shell.execute_reply":"2022-02-01T13:20:53.495687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = roberta_model.fit(\n    x=x_train['input_ids'],\n    y= y_train,\n    validation_data=(x_val['input_ids'], y_val),\n    batch_size=256,\n    epochs=10,\n    #callbacks = callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:17:27.041368Z","iopub.execute_input":"2022-02-01T12:17:27.041615Z","iopub.status.idle":"2022-02-01T12:23:00.629625Z","shell.execute_reply.started":"2022-02-01T12:17:27.04158Z","shell.execute_reply":"2022-02-01T12:23:00.628516Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 6. Evaluating performance of both models on val set","metadata":{}},{"cell_type":"code","source":"y_val_pred = model.predict(x_val)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_val_pred_max = np.argmax(y_val_pred , axis = 1)\ny_val_gt_max = np.argmax(y_val , axis = 1)\n\nprint(y_val_pred_max.shape)\nprint(y_val_gt_max.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nreport = classification_report(y_val_pred_max, y_val_gt_max)\n\nprint(report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nprint(sns.heatmap(confusion_matrix(y_val_gt_max , y_val_pred_max) , annot=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-01T12:25:06.120659Z","iopub.execute_input":"2022-02-01T12:25:06.121449Z","iopub.status.idle":"2022-02-01T12:25:06.408203Z","shell.execute_reply.started":"2022-02-01T12:25:06.121407Z","shell.execute_reply":"2022-02-01T12:25:06.407525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}