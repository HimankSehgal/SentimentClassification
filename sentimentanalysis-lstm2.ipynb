{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"In this project we'll be performing sentiment analysis on Rotten Tomatoes Dataset whose dataset has been attached in this repo\n\nThe main task corresponds to a multi-class text classification on Movie Reviews Competition and the dataset contains 156.060 instances for training, whereas the testing set contains 66.292 from which we have to classify among 5 classes. The sentiment labels are:\n\n0 → Negative      </br>\n1 → Somewhat negative  </br>\n2 → Neutral </br>\n3 → Somewhat positive </br>\n4 → Positive </br>\n\n\n\nWe will be comparing performance of BiLSTM Model","metadata":{}},{"cell_type":"markdown","source":"# Steps to be followed\n\n1. Importing necessary libraries\n2. Opening the train and test dataset in the form of pandas dataframe and perform exploratory data analysis on train data \n3. Taking the train data and splitting it into train and val dataset using <tt>**StratifiedSplit()**</tt>\n4. Performing text preprocessing like pattern matching using regex , word tokenization , lemmatization\n5. Building the LSTM model and training it\n6. Analysing the performance of model\n7. Future improvments","metadata":{}},{"cell_type":"markdown","source":"### Step 1. Importing necessary libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n## other libraries will be imported as and when required","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:11.539811Z","iopub.execute_input":"2022-02-02T06:03:11.540145Z","iopub.status.idle":"2022-02-02T06:03:12.535161Z","shell.execute_reply.started":"2022-02-02T06:03:11.540064Z","shell.execute_reply":"2022-02-02T06:03:12.534428Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Opening the dataset and performing EDA","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/sentimentdata/train.tsv/train.tsv' , sep='\\t')\ntest_df = pd.read_csv('../input/sentimentdata/test.tsv/test.tsv' , sep = '\\t')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:12.539626Z","iopub.execute_input":"2022-02-02T06:03:12.541817Z","iopub.status.idle":"2022-02-02T06:03:13.006571Z","shell.execute_reply.started":"2022-02-02T06:03:12.541775Z","shell.execute_reply":"2022-02-02T06:03:13.005855Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:13.007845Z","iopub.execute_input":"2022-02-02T06:03:13.008105Z","iopub.status.idle":"2022-02-02T06:03:13.026789Z","shell.execute_reply.started":"2022-02-02T06:03:13.008071Z","shell.execute_reply":"2022-02-02T06:03:13.026087Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"print(train_df.shape)\n#print(train_df.info)\nprint(train_df.columns)\nprint(train_df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:13.030481Z","iopub.execute_input":"2022-02-02T06:03:13.030666Z","iopub.status.idle":"2022-02-02T06:03:13.054781Z","shell.execute_reply.started":"2022-02-02T06:03:13.030644Z","shell.execute_reply":"2022-02-02T06:03:13.054003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df2=train_df.copy(deep=True)\npie1=pd.DataFrame(df2['Sentiment'].replace(0,'Negative').replace(1,'Somewhat negative').replace(2,'Neutral').replace(3,'Somewhat positive').replace(4,'Positive').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Sentiment Class',y = 'Sentiment', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:13.056213Z","iopub.execute_input":"2022-02-02T06:03:13.056466Z","iopub.status.idle":"2022-02-02T06:03:13.396020Z","shell.execute_reply.started":"2022-02-02T06:03:13.056433Z","shell.execute_reply":"2022-02-02T06:03:13.395229Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5,figsize=(25,8))\nax1.hist(train_df[train_df['Sentiment'] == 0]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='b')\nax1.set_title('Negative Reviews')\n\nax2.hist(train_df[train_df['Sentiment'] == 1]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='r')\nax2.set_title('Somewhat Negative Reviews')\n\nax3.hist(train_df[train_df['Sentiment'] == 2]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='g')\nax3.set_title('Neutral Reviews')\n\nax4.hist(train_df[train_df['Sentiment'] == 3]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='y')\nax4.set_title('Somewhat Positive Reviews')\n\nax5.hist(train_df[train_df['Sentiment'] == 4]['Phrase'].str.split().map(lambda x: len(x)), bins=50, color='k')\nax5.set_title('Positive Reviews')\n\nf.suptitle('Histogram number of words in reviews')","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:13.400158Z","iopub.execute_input":"2022-02-02T06:03:13.400625Z","iopub.status.idle":"2022-02-02T06:03:14.927498Z","shell.execute_reply.started":"2022-02-02T06:03:13.400594Z","shell.execute_reply":"2022-02-02T06:03:14.926829Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df['Phrase'].str.split().map(lambda x: len(x)).max()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:14.928813Z","iopub.execute_input":"2022-02-02T06:03:14.929223Z","iopub.status.idle":"2022-02-02T06:03:15.471553Z","shell.execute_reply.started":"2022-02-02T06:03:14.929185Z","shell.execute_reply":"2022-02-02T06:03:15.470877Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"***Insights*** <br>\nThrough these graphs we can see that most reviews of any class are of shorter length, around 5-20. But max length is 52 \nEffectively was 52 words, this means if we would Tokenize by word the max_length should be 52, however as transformers consider sub-words tokenization such number could be increased depending on the words being used which can increase such length to 60 or even more, thus we have to take that into account when modeling as it could cause our model to take significatively a long time to train, therefore we have to find a trade-off between training time and performance.","metadata":{}},{"cell_type":"code","source":"\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=20)\nprint('Number of sentences which contain more than 20 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=30)\nprint('Number of sentences which contain more than 30 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=40)\nprint('Number of sentences which contain more than 40 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))>=50)\nprint('Number of sentences which contain more than 50 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\ndf=pd.DataFrame(train_df['Phrase'].str.split().map(lambda x: len(x))==52)\nprint('Number of sentences which contain 52 words: ', df.loc[df['Phrase']==True].shape[0])\nprint(' ')\n#dfff.loc[dfff['Phrase']==True]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:15.472730Z","iopub.execute_input":"2022-02-02T06:03:15.473022Z","iopub.status.idle":"2022-02-02T06:03:17.730596Z","shell.execute_reply.started":"2022-02-02T06:03:15.472953Z","shell.execute_reply":"2022-02-02T06:03:17.729741Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"***Insights*** <br>\nWe can remove sentences which have length more than 40 words and they won't contribute much but removing them can help to boost computation","metadata":{}},{"cell_type":"code","source":"train_df['len'] = train_df['Phrase'].str.split().map(lambda x: len(x))\nprint(train_df.shape)\n\ntrain_df = train_df[train_df['len'] <40 ]\nprint(train_df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:17.731973Z","iopub.execute_input":"2022-02-02T06:03:17.732321Z","iopub.status.idle":"2022-02-02T06:03:18.268253Z","shell.execute_reply.started":"2022-02-02T06:03:17.732285Z","shell.execute_reply":"2022-02-02T06:03:18.267317Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"156060 - 155708","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:18.271602Z","iopub.execute_input":"2022-02-02T06:03:18.271833Z","iopub.status.idle":"2022-02-02T06:03:18.278687Z","shell.execute_reply.started":"2022-02-02T06:03:18.271798Z","shell.execute_reply":"2022-02-02T06:03:18.277819Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Step 3. Taking the train data and splitting it into train and val dataset using <tt>**StratifiedSplit()**</tt>","metadata":{}},{"cell_type":"code","source":"train_df['Sentiment'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:18.279935Z","iopub.execute_input":"2022-02-02T06:03:18.280420Z","iopub.status.idle":"2022-02-02T06:03:18.292135Z","shell.execute_reply.started":"2022-02-02T06:03:18.280280Z","shell.execute_reply":"2022-02-02T06:03:18.291432Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"There is an imbalance . So we cannot do random split, We'll do <tt>**StratifiedSplit()**</tt> to ensure distribution is same in splits\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nX = train_df.drop('Sentiment',axis=1)\ny = train_df['Sentiment']\nsss = StratifiedShuffleSplit(n_splits=2, test_size=0.1, random_state=0) #test size of 10% \n\nfor train_index, test_index in sss.split(X , y):  \n    X_train = X.iloc[train_index]\n    y_train = y.iloc[train_index]  \n    X_val = X.iloc[test_index]\n    y_val = y.iloc[test_index]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:18.293557Z","iopub.execute_input":"2022-02-02T06:03:18.293748Z","iopub.status.idle":"2022-02-02T06:03:18.593162Z","shell.execute_reply.started":"2022-02-02T06:03:18.293724Z","shell.execute_reply":"2022-02-02T06:03:18.592348Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### Step 4. Performing text preprocessing like pattern matching using regex , word tokenization , lemmatization","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:18.594678Z","iopub.execute_input":"2022-02-02T06:03:18.595011Z","iopub.status.idle":"2022-02-02T06:03:19.034911Z","shell.execute_reply.started":"2022-02-02T06:03:18.594927Z","shell.execute_reply":"2022-02-02T06:03:19.034225Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def sentence_cleaning(df):\n    sentence = []\n    for sent in df['Phrase']:\n        \n        text = re.sub(\"[^a-zA-Z]\",\" \",sent)\n        \n        word = word_tokenize(text.lower())\n        \n        lemmatizer = WordNetLemmatizer()\n        \n        lemm_word = [lemmatizer.lemmatize(i) for i in word]\n        \n        sentence.append(lemm_word)\n    return (sentence)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:19.036260Z","iopub.execute_input":"2022-02-02T06:03:19.036526Z","iopub.status.idle":"2022-02-02T06:03:19.042427Z","shell.execute_reply.started":"2022-02-02T06:03:19.036494Z","shell.execute_reply":"2022-02-02T06:03:19.041525Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:19.044029Z","iopub.execute_input":"2022-02-02T06:03:19.044353Z","iopub.status.idle":"2022-02-02T06:03:24.182469Z","shell.execute_reply.started":"2022-02-02T06:03:19.044318Z","shell.execute_reply":"2022-02-02T06:03:24.181740Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(y_train.shape)\nprint(y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:24.183642Z","iopub.execute_input":"2022-02-02T06:03:24.183903Z","iopub.status.idle":"2022-02-02T06:03:24.189571Z","shell.execute_reply.started":"2022-02-02T06:03:24.183870Z","shell.execute_reply":"2022-02-02T06:03:24.188908Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"x_train = sentence_cleaning(X_train)\nx_val = sentence_cleaning(X_val)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:24.190799Z","iopub.execute_input":"2022-02-02T06:03:24.191176Z","iopub.status.idle":"2022-02-02T06:03:51.095920Z","shell.execute_reply.started":"2022-02-02T06:03:24.191140Z","shell.execute_reply":"2022-02-02T06:03:51.095226Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"unique_words = set()\nmax_len = 0\nfor sent in tqdm(x_train):\n    unique_words.update(sent)\n    if(max_len < len(sent)):\n        max_len = len(sent)\n        sentence = sent","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.097374Z","iopub.execute_input":"2022-02-02T06:03:51.097630Z","iopub.status.idle":"2022-02-02T06:03:51.278278Z","shell.execute_reply.started":"2022-02-02T06:03:51.097597Z","shell.execute_reply":"2022-02-02T06:03:51.277491Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"len(list(unique_words))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.279571Z","iopub.execute_input":"2022-02-02T06:03:51.279996Z","iopub.status.idle":"2022-02-02T06:03:51.286683Z","shell.execute_reply.started":"2022-02-02T06:03:51.279956Z","shell.execute_reply":"2022-02-02T06:03:51.285786Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"max_len","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.288094Z","iopub.execute_input":"2022-02-02T06:03:51.288567Z","iopub.status.idle":"2022-02-02T06:03:51.297491Z","shell.execute_reply.started":"2022-02-02T06:03:51.288517Z","shell.execute_reply":"2022-02-02T06:03:51.296644Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"vocabulary = len(list(unique_words))\n#vocab_size = len(tokenizer.word_index) + 1\noov = '<OOV>'\nembedding_dim = 300\npadding = 'post'\ntrunc = 'post'","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.298931Z","iopub.execute_input":"2022-02-02T06:03:51.299485Z","iopub.status.idle":"2022-02-02T06:03:51.305465Z","shell.execute_reply.started":"2022-02-02T06:03:51.299431Z","shell.execute_reply":"2022-02-02T06:03:51.304624Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.308128Z","iopub.execute_input":"2022-02-02T06:03:51.308754Z","iopub.status.idle":"2022-02-02T06:03:51.314975Z","shell.execute_reply.started":"2022-02-02T06:03:51.308713Z","shell.execute_reply":"2022-02-02T06:03:51.314217Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(x_train[:5])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.316543Z","iopub.execute_input":"2022-02-02T06:03:51.316907Z","iopub.status.idle":"2022-02-02T06:03:51.325836Z","shell.execute_reply.started":"2022-02-02T06:03:51.316870Z","shell.execute_reply":"2022-02-02T06:03:51.324709Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words = vocabulary,oov_token = oov,char_level = False)\ntokenizer.fit_on_texts(list(x_train))\n\n\nx_train = tokenizer.texts_to_sequences(x_train)\nx_train = pad_sequences(x_train,maxlen = max_len,padding=padding,truncating = trunc)\n\n\nx_val = tokenizer.texts_to_sequences(x_val)\nx_val = pad_sequences(x_val,maxlen = max_len,padding=padding,truncating = trunc)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:51.327179Z","iopub.execute_input":"2022-02-02T06:03:51.327513Z","iopub.status.idle":"2022-02-02T06:03:53.666936Z","shell.execute_reply.started":"2022-02-02T06:03:51.327474Z","shell.execute_reply":"2022-02-02T06:03:53.666111Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)\nprint(x_val.shape)\nprint(x_train[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:53.668482Z","iopub.execute_input":"2022-02-02T06:03:53.668971Z","iopub.status.idle":"2022-02-02T06:03:53.681577Z","shell.execute_reply.started":"2022-02-02T06:03:53.668934Z","shell.execute_reply":"2022-02-02T06:03:53.680860Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Step 5. Buidling LSTM Model","metadata":{}},{"cell_type":"code","source":"# import gensim.models.keyedvectors as word2vec\n# w2vModel = word2vec.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=50000)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T05:53:20.770670Z","iopub.execute_input":"2022-02-02T05:53:20.771334Z","iopub.status.idle":"2022-02-02T05:53:20.775643Z","shell.execute_reply.started":"2022-02-02T05:53:20.771291Z","shell.execute_reply":"2022-02-02T05:53:20.774650Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from keras.models import Model,Input\nfrom keras.layers import Dense,Bidirectional,Activation,Dropout,LSTM,Embedding\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau , ModelCheckpoint , EarlyStopping","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:53.688099Z","iopub.execute_input":"2022-02-02T06:03:53.690096Z","iopub.status.idle":"2022-02-02T06:03:53.697787Z","shell.execute_reply.started":"2022-02-02T06:03:53.690058Z","shell.execute_reply":"2022-02-02T06:03:53.697090Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"input_vec = Input(shape = (41,))\nx = Embedding(vocabulary,embedding_dim,input_length = max_len)(input_vec)\nx = Bidirectional(LSTM(128, dropout = 0.8, recurrent_dropout=0.8, return_sequences=True))(x)\nx = Bidirectional(LSTM(128, dropout = 0.8, recurrent_dropout=0.8, return_sequences=False))(x)\nx = Dense(50 , activation = 'relu')(x)\nx = Dropout(0.4)(x)\nx = Dense(5 , activation = 'softmax' , name = 'Sentiment')(x)\nmodel = Model(inputs = [input_vec] , outputs = [x])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:53.702113Z","iopub.execute_input":"2022-02-02T06:03:53.704482Z","iopub.status.idle":"2022-02-02T06:03:56.811172Z","shell.execute_reply.started":"2022-02-02T06:03:53.704445Z","shell.execute_reply":"2022-02-02T06:03:56.810446Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:56.812595Z","iopub.execute_input":"2022-02-02T06:03:56.812851Z","iopub.status.idle":"2022-02-02T06:03:56.823405Z","shell.execute_reply.started":"2022-02-02T06:03:56.812816Z","shell.execute_reply":"2022-02-02T06:03:56.822561Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.compile(loss = 'categorical_crossentropy',\n             optimizer = 'adam',\n             metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:56.825121Z","iopub.execute_input":"2022-02-02T06:03:56.825755Z","iopub.status.idle":"2022-02-02T06:03:56.838259Z","shell.execute_reply.started":"2022-02-02T06:03:56.825718Z","shell.execute_reply":"2022-02-02T06:03:56.837540Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":" callbacks = [\n     EarlyStopping(patience=5),\n     ReduceLROnPlateau(factor=0.3, patience=3, min_lr=0.00001 ),\n     ModelCheckpoint('lstm_model.h5')\n ]","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:56.839770Z","iopub.execute_input":"2022-02-02T06:03:56.840063Z","iopub.status.idle":"2022-02-02T06:03:56.846321Z","shell.execute_reply.started":"2022-02-02T06:03:56.840003Z","shell.execute_reply":"2022-02-02T06:03:56.845521Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train,y_train,validation_data = (x_val, y_val),epochs = 30\n          ,batch_size = 4196,verbose = 1,callbacks = callbacks)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:03:56.847489Z","iopub.execute_input":"2022-02-02T06:03:56.848280Z","iopub.status.idle":"2022-02-02T06:15:09.222747Z","shell.execute_reply.started":"2022-02-02T06:03:56.848238Z","shell.execute_reply":"2022-02-02T06:15:09.221914Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## Step 6. Evaluating model's performance","metadata":{}},{"cell_type":"code","source":"y_val_pred = model.predict(x_val)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:15:59.640536Z","iopub.execute_input":"2022-02-02T06:15:59.640794Z","iopub.status.idle":"2022-02-02T06:16:20.755577Z","shell.execute_reply.started":"2022-02-02T06:15:59.640763Z","shell.execute_reply":"2022-02-02T06:16:20.754665Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"y_val_pred.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:16:20.757529Z","iopub.execute_input":"2022-02-02T06:16:20.757798Z","iopub.status.idle":"2022-02-02T06:16:20.765885Z","shell.execute_reply.started":"2022-02-02T06:16:20.757748Z","shell.execute_reply":"2022-02-02T06:16:20.765058Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"y_val_pred_max = np.argmax(y_val_pred , axis = 1)\ny_val_gt_max = np.argmax(y_val , axis = 1)\n\nprint(y_val_pred_max.shape)\nprint(y_val_gt_max.shape)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:16:20.768232Z","iopub.execute_input":"2022-02-02T06:16:20.768751Z","iopub.status.idle":"2022-02-02T06:16:20.775383Z","shell.execute_reply.started":"2022-02-02T06:16:20.768713Z","shell.execute_reply":"2022-02-02T06:16:20.774606Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report,confusion_matrix\nreport = classification_report(y_val_pred_max, y_val_gt_max)\n\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:16:28.676251Z","iopub.execute_input":"2022-02-02T06:16:28.676511Z","iopub.status.idle":"2022-02-02T06:16:28.706828Z","shell.execute_reply.started":"2022-02-02T06:16:28.676481Z","shell.execute_reply":"2022-02-02T06:16:28.706110Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nprint(sns.heatmap(confusion_matrix(y_val_gt_max , y_val_pred_max) , annot=True))","metadata":{"execution":{"iopub.status.busy":"2022-02-02T06:16:32.815405Z","iopub.execute_input":"2022-02-02T06:16:32.816207Z","iopub.status.idle":"2022-02-02T06:16:33.114958Z","shell.execute_reply.started":"2022-02-02T06:16:32.816169Z","shell.execute_reply":"2022-02-02T06:16:33.114284Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"## The model is having an f1 score of 0.67 which can be further improved if we use the pretrained word embeddings","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}